{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h5T6rJA3w0XL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Flatten,Reshape\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import csv\n",
    "import nltk\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yif2s43w-Amb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.layers import Input,Concatenate,Dropout,Dense,BatchNormalization,Conv1D\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import scipy\n",
    "from tensorflow.keras.initializers import he_normal,glorot_normal\n",
    "from tensorflow.keras.regularizers import l1,l2\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler,ReduceLROnPlateau\n",
    "from time import time\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input,BatchNormalization,Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "import random as rn\n",
    "import string\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.initializers import glorot_uniform,glorot_normal\n",
    "from tensorflow.keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "coVpb7Lg-T1R",
    "outputId": "c3bb1cfa-5ba8-4eb9-bfab-2203324f2369"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>compound</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>com_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>i think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9363</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.852</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>when a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-0.9194</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.798</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  compound  negative  \\\n",
       "0     3.5         3.0          3.0      4.0          3.0    0.9363     0.050   \n",
       "1     2.5         3.0          2.0      2.0          2.5   -0.9194     0.111   \n",
       "\n",
       "   positive  neutral  com_len  \n",
       "0     0.098    0.852      264  \n",
       "1     0.091    0.798      536  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading the training data\n",
    "import pandas as pd\n",
    "training_data=pd.read_csv(\"./train.csv\")\n",
    "display(train.head(2))\n",
    "#display(test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gl8fSjkIxPiP",
    "outputId": "a3965b3c-7b32-461c-ccb9-778f582ca8ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 1045.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \"\"\"\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "def decontractions(phrase):\n",
    "   \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "#preprocessing \n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "stopwords = stopwords.words('english')\n",
    "def preprocess(text_col,stopword):\n",
    "    preprocessed = []\n",
    "    for sentence in tqdm(text_col.values):\n",
    "        # Replace \"carriage return\" with \"space\".\n",
    "        sentence=str(sentence)\n",
    "        sent = sentence.replace('\\\\r', ' ')\n",
    "        # Replace \"quotes\" with \"space\".\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        # Replace \"line feed\" with \"space\".\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "         # Replace characters between words with \"space\".\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        #remove stop words\n",
    "        #decontraction\n",
    "        sent=decontractions(sent)\n",
    "        if stopword:\n",
    "            sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
    "        else:\n",
    "            sent = ' '.join(e for e in sent.split())\n",
    "        # to lowercase\n",
    "        preprocessed.append(sent.lower().strip())\n",
    "    return preprocessed\n",
    "training_data['full_text']=preprocess(training_data['full_text'],stopword=False)\n",
    "#test['full_text']=preprocess(test['full_text'],stopword=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>i think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>when a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>dear principal if u change the school policy o...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>the best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  dear principal if u change the school policy o...       3.0   \n",
       "3  003885A45F42  the best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  \n",
       "2     3.5         3.0          3.0      3.0          2.5  \n",
       "3     4.5         4.5          4.5      4.0          5.0  \n",
       "4     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YezopuRjxs0J"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "      <th>compound</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>com_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>i think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9363</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.852</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>when a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-0.9194</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.798</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>dear principal if u change the school policy o...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.9271</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.836</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>the best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.698</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.9868</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.836</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  i think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  when a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  dear principal if u change the school policy o...       3.0   \n",
       "3  003885A45F42  the best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  compound  negative  \\\n",
       "0     3.5         3.0          3.0      4.0          3.0    0.9363     0.050   \n",
       "1     2.5         3.0          2.0      2.0          2.5   -0.9194     0.111   \n",
       "2     3.5         3.0          3.0      3.0          2.5    0.9271     0.054   \n",
       "3     4.5         4.5          4.5      4.0          5.0    0.9996     0.051   \n",
       "4     3.0         3.0          3.0      2.5          2.5    0.9868     0.000   \n",
       "\n",
       "   positive  neutral  com_len  \n",
       "0     0.098    0.852      264  \n",
       "1     0.091    0.798      536  \n",
       "2     0.110    0.836      330  \n",
       "3     0.252    0.698      756  \n",
       "4     0.164    0.836      234  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cehcking word count \n",
    "train['length']=train['full_text'].apply(lambda x:len(x.split()))\n",
    "test['length']=test['full_text'].apply(lambda x:len(x.split()))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-17b3TaLxxTF",
    "outputId": "b72ca6a1-c6a8-42b5-fbd0-fc2d4a5737ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 1) (3128, 6)\n"
     ]
    }
   ],
   "source": [
    "y=training_data[['cohesion','syntax','vocabulary','phraseology','grammar','conventions']]\n",
    "X=training_data.drop(['text_id','cohesion','syntax','vocabulary','phraseology','grammar','conventions'],axis=1)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.20)\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFyKJfx4x2dB",
    "outputId": "367e8b83-84d7-449b-dcb9-6bd275645a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 200) (783, 200) (3, 200)\n"
     ]
    }
   ],
   "source": [
    "#padding \n",
    "\n",
    "def pad_text(text,tokenizer,max_len):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(text),maxlen=max_len,padding='post')\n",
    "\n",
    "\n",
    "def text_padding(train,test,test_final,max_len):\n",
    "    vocab=5000\n",
    "    token=Tokenizer()\n",
    "    token.fit_on_texts(train)\n",
    "    padded_train_text=pad_text(train,token,max_len)\n",
    "    padded_test_text=pad_text(test,token,max_len)\n",
    "    padded_test_final_text=pad_text(test_final,token,max_len)\n",
    "    return padded_train_text,padded_test_text,padded_test_final_text,token\n",
    "comm_len=200\n",
    "train_com_pad,test_com_pad,test_final_pad,token_com= text_padding(X_train['full_text'],X_test['full_text'],test['full_text'],comm_len)\n",
    "print(train_com_pad.shape,test_com_pad.shape,test_final_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 446    3 3950 ...  274   12   23]\n",
      " [  12 2510   72 ... 5965   12 1188]\n",
      " [  77   58   19 ...   15  105   11]\n",
      " ...\n",
      " [  12    2  327 ...  116  619  390]\n",
      " [ 569   83  454 ...    5  636   85]\n",
      " [  47   66   26 ...   36   11  238]]\n"
     ]
    }
   ],
   "source": [
    "print(train_com_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "eraKdj-rx-W_"
   },
   "outputs": [],
   "source": [
    "def generate_embedding_matrix(token):\n",
    "    embedding_path='./crawl-300d-2M.vec' #pre trained FastText English word vectors released by FB\n",
    "    embedding_size=300\n",
    "    vocab_size=5000\n",
    "    embedding_index={}\n",
    "    with open(embedding_path, 'r',encoding=\"utf8\") as f:\n",
    "         for line in f:\n",
    "                values = line.rstrip().rsplit(' ')\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embedding_index[word] = coefs\n",
    "    num_words = len(token.word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "    for word, i in token.word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNBuWe-gyR1d",
    "outputId": "277609e5-75b0-4584-a2d0-8a589935e169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18974, 300)\n"
     ]
    }
   ],
   "source": [
    "# generating the embedding matrix containing weigths\n",
    "embedding_comm = generate_embedding_matrix(token_com)\n",
    "print(embedding_comm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73hT3AYa52dy",
    "outputId": "c43dd834-35e1-4249-f437-a9e093503907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128, 6) (783, 6) (3, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_s.shape,test_s.shape,test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s=np.array(y_train)\n",
    "test_s=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "vqnevNmx56pZ"
   },
   "outputs": [],
   "source": [
    "X_train=[train_com_pad,train_com_pad,train_s]\n",
    "X_test=[test_com_pad,test_com_pad,test_s]\n",
    "#X_test_final=[test_final_pad,test_final_pad,test_final]\n",
    "y_train=np.array(y_train,dtype=np.float64)\n",
    "y_test=np.array(y_test, dtype=np.float64)\n",
    "def mcrmse(y_true, y_pred):\n",
    "    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=0)\n",
    "    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "vqrDfRu76DkI"
   },
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "def LSTM_CNN1D(comm_len,token_com):\n",
    "    drop_lstm = 0.25\n",
    "    drop_dense = 0.25\n",
    "    num_lstm=150\n",
    "    input_1 = Input(shape=(comm_len,),name = 'input_comment_1')\n",
    "    embedding_layer_1 = Embedding(len(token_com.word_index) + 1,300,weights=[embedding_comm],input_length=comm_len,trainable=False,dtype=tf.float32)(input_1)\n",
    "    conv_1_1 = Conv1D(64,3,strides=1, padding='same',activation='relu')(embedding_layer_1)\n",
    "    lstm_1_1 = LSTM(64,dropout=drop_lstm,return_sequences=True,dtype=tf.float32)(embedding_layer_1)\n",
    "    concate_1 = keras.layers.Concatenate(axis=-1)([conv_1_1, lstm_1_1])\n",
    "    flatten_1 = Flatten()(concate_1)\n",
    "\n",
    "    # creating layers for parent comment data\n",
    "    input_2 = Input(shape=(comm_len,),name = 'input_comment_2')\n",
    "    embedding_layer_2 = Embedding(len(token_com.word_index) + 1,300,weights=[embedding_comm],input_length=comm_len,trainable=False,dtype=tf.float32)(input_2)\n",
    "    conv_1_1 = Conv1D(128,3,strides=1, padding='same',activation='relu')(embedding_layer_2)\n",
    "    lstm_1_2 =LSTM(128,dropout=drop_lstm,return_sequences=True,dtype=tf.float32)(embedding_layer_2)\n",
    "    concate_2 = keras.layers.Concatenate(axis=-1)([conv_1_1, lstm_1_2])\n",
    "    flatten_2 = Flatten()(concate_2)\n",
    "\n",
    "    # creating layers for numerical columns\n",
    "    input_3 = Input(shape=(6,), name = \"input_numerical\")\n",
    "    dense_num_layer = Dense(128, activation = \"relu\")(input_3)\n",
    "\n",
    "    concatenated_layer = keras.layers.concatenate([flatten_1,flatten_2,dense_num_layer],axis=-1)\n",
    "\n",
    "    # creating further layers\n",
    "    x = Dense(128, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(concatenated_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(16, activation = 'relu',kernel_initializer=glorot_uniform(seed=42))(x)\n",
    "    output = Dense(6,activation='linear')(x)\n",
    "    model = Model(inputs = [input_1,input_2,input_3], outputs = [output])\n",
    "    model.compile(optimizer=adam, loss = mcrmse, metrics = mcrmse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ec8AGYpG6SYm",
    "outputId": "3cac99b7-601c-4e9d-cc4c-d6b743e5588a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_comment_1 (InputLayer)   [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " input_comment_2 (InputLayer)   [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 200, 300)     5692200     ['input_comment_1[0][0]']        \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 200, 300)     5692200     ['input_comment_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 200, 64)      57664       ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 200, 64)      93440       ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 200, 128)     115328      ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 200, 128)     219648      ['embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 200, 128)     0           ['conv1d_6[0][0]',               \n",
      "                                                                  'lstm_6[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 200, 256)     0           ['conv1d_7[0][0]',               \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " input_numerical (InputLayer)   [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 25600)        0           ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 51200)        0           ['concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 128)          896         ['input_numerical[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 76928)        0           ['flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]',              \n",
      "                                                                  'dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 128)          9846912     ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 128)         512         ['dense_19[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64)           8256        ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 64)          256         ['dense_20[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 32)           2080        ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32)          128         ['dense_21[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 16)           528         ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 6)            102         ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,730,150\n",
      "Trainable params: 10,345,302\n",
      "Non-trainable params: 11,384,848\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_CNN1D(comm_len,token_com)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "7jeLdKTq6rD1"
   },
   "outputs": [],
   "source": [
    "#reduce_lr reduces the learning rate when the metric has stoppes improving for 2 epochs. \n",
    "#Using EarlyStopping to stop the calculation upon reaching enough accuracy\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_mcrmse', factor = 0.25, patience = 2, verbose = 1)\n",
    "earlystop = EarlyStopping(monitor = 'val_mcrmse',  mode=\"min\",min_delta = 0, patience = 25,verbose = 1)\n",
    "callbacks = [reduce_lr,earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crrtOMqr6xJS",
    "outputId": "a7e2087b-140f-4f2e-9b46-a3542eb73ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "98/98 [==============================] - 82s 620ms/step - loss: 1.5571 - mcrmse: 1.5551 - val_loss: 1.8791 - val_mcrmse: 1.8823 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "98/98 [==============================] - 44s 450ms/step - loss: 0.4874 - mcrmse: 0.4874 - val_loss: 1.1480 - val_mcrmse: 1.1484 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "98/98 [==============================] - 52s 527ms/step - loss: 0.4295 - mcrmse: 0.4293 - val_loss: 0.5263 - val_mcrmse: 0.5270 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "98/98 [==============================] - 60s 611ms/step - loss: 0.3865 - mcrmse: 0.3866 - val_loss: 0.4638 - val_mcrmse: 0.4640 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "98/98 [==============================] - 55s 564ms/step - loss: 0.3438 - mcrmse: 0.3439 - val_loss: 0.4215 - val_mcrmse: 0.4212 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "98/98 [==============================] - 58s 593ms/step - loss: 0.3310 - mcrmse: 0.3312 - val_loss: 0.4853 - val_mcrmse: 0.4859 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "98/98 [==============================] - 55s 557ms/step - loss: 0.3153 - mcrmse: 0.3154 - val_loss: 0.3980 - val_mcrmse: 0.3980 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "98/98 [==============================] - 51s 521ms/step - loss: 0.3064 - mcrmse: 0.3064 - val_loss: 0.3298 - val_mcrmse: 0.3298 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "98/98 [==============================] - 57s 585ms/step - loss: 0.2820 - mcrmse: 0.2821 - val_loss: 0.3409 - val_mcrmse: 0.3413 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2726 - mcrmse: 0.2728\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "98/98 [==============================] - 72s 732ms/step - loss: 0.2726 - mcrmse: 0.2728 - val_loss: 0.3466 - val_mcrmse: 0.3462 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "98/98 [==============================] - 212s 2s/step - loss: 0.2618 - mcrmse: 0.2618 - val_loss: 0.2225 - val_mcrmse: 0.2224 - lr: 2.5000e-04\n",
      "Epoch 12/300\n",
      "98/98 [==============================] - 281s 3s/step - loss: 0.2462 - mcrmse: 0.2461 - val_loss: 0.1989 - val_mcrmse: 0.1985 - lr: 2.5000e-04\n",
      "Epoch 13/300\n",
      "98/98 [==============================] - 157s 2s/step - loss: 0.2455 - mcrmse: 0.2454 - val_loss: 0.1784 - val_mcrmse: 0.1786 - lr: 2.5000e-04\n",
      "Epoch 14/300\n",
      "98/98 [==============================] - 41s 418ms/step - loss: 0.2386 - mcrmse: 0.2388 - val_loss: 0.1984 - val_mcrmse: 0.1986 - lr: 2.5000e-04\n",
      "Epoch 15/300\n",
      "98/98 [==============================] - 48s 488ms/step - loss: 0.2303 - mcrmse: 0.2305 - val_loss: 0.1734 - val_mcrmse: 0.1734 - lr: 2.5000e-04\n",
      "Epoch 16/300\n",
      "98/98 [==============================] - 51s 521ms/step - loss: 0.2428 - mcrmse: 0.2429 - val_loss: 0.1882 - val_mcrmse: 0.1888 - lr: 2.5000e-04\n",
      "Epoch 17/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2374 - mcrmse: 0.2375\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "98/98 [==============================] - 55s 563ms/step - loss: 0.2374 - mcrmse: 0.2375 - val_loss: 0.1933 - val_mcrmse: 0.1935 - lr: 2.5000e-04\n",
      "Epoch 18/300\n",
      "98/98 [==============================] - 60s 608ms/step - loss: 0.2339 - mcrmse: 0.2339 - val_loss: 0.1537 - val_mcrmse: 0.1539 - lr: 6.2500e-05\n",
      "Epoch 19/300\n",
      "98/98 [==============================] - 54s 552ms/step - loss: 0.2258 - mcrmse: 0.2259 - val_loss: 0.1490 - val_mcrmse: 0.1493 - lr: 6.2500e-05\n",
      "Epoch 20/300\n",
      "98/98 [==============================] - 53s 540ms/step - loss: 0.2255 - mcrmse: 0.2256 - val_loss: 0.1506 - val_mcrmse: 0.1508 - lr: 6.2500e-05\n",
      "Epoch 21/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2177 - mcrmse: 0.2177\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "98/98 [==============================] - 55s 561ms/step - loss: 0.2177 - mcrmse: 0.2177 - val_loss: 0.1499 - val_mcrmse: 0.1501 - lr: 6.2500e-05\n",
      "Epoch 22/300\n",
      "98/98 [==============================] - 53s 545ms/step - loss: 0.2219 - mcrmse: 0.2219 - val_loss: 0.1459 - val_mcrmse: 0.1461 - lr: 1.5625e-05\n",
      "Epoch 23/300\n",
      "98/98 [==============================] - 52s 533ms/step - loss: 0.2234 - mcrmse: 0.2237 - val_loss: 0.1450 - val_mcrmse: 0.1452 - lr: 1.5625e-05\n",
      "Epoch 24/300\n",
      "98/98 [==============================] - 55s 558ms/step - loss: 0.2176 - mcrmse: 0.2178 - val_loss: 0.1452 - val_mcrmse: 0.1454 - lr: 1.5625e-05\n",
      "Epoch 25/300\n",
      "98/98 [==============================] - 62s 630ms/step - loss: 0.2161 - mcrmse: 0.2161 - val_loss: 0.1433 - val_mcrmse: 0.1436 - lr: 1.5625e-05\n",
      "Epoch 26/300\n",
      "98/98 [==============================] - 53s 545ms/step - loss: 0.2171 - mcrmse: 0.2174 - val_loss: 0.1437 - val_mcrmse: 0.1440 - lr: 1.5625e-05\n",
      "Epoch 27/300\n",
      "98/98 [==============================] - 53s 541ms/step - loss: 0.2268 - mcrmse: 0.2267 - val_loss: 0.1431 - val_mcrmse: 0.1433 - lr: 1.5625e-05\n",
      "Epoch 28/300\n",
      "98/98 [==============================] - 54s 549ms/step - loss: 0.2146 - mcrmse: 0.2147 - val_loss: 0.1438 - val_mcrmse: 0.1441 - lr: 1.5625e-05\n",
      "Epoch 29/300\n",
      "98/98 [==============================] - 52s 531ms/step - loss: 0.2152 - mcrmse: 0.2154 - val_loss: 0.1424 - val_mcrmse: 0.1427 - lr: 1.5625e-05\n",
      "Epoch 30/300\n",
      "98/98 [==============================] - 52s 529ms/step - loss: 0.2146 - mcrmse: 0.2146 - val_loss: 0.1420 - val_mcrmse: 0.1422 - lr: 1.5625e-05\n",
      "Epoch 31/300\n",
      "98/98 [==============================] - 60s 612ms/step - loss: 0.2182 - mcrmse: 0.2183 - val_loss: 0.1416 - val_mcrmse: 0.1419 - lr: 1.5625e-05\n",
      "Epoch 32/300\n",
      "98/98 [==============================] - 56s 577ms/step - loss: 0.2114 - mcrmse: 0.2114 - val_loss: 0.1418 - val_mcrmse: 0.1420 - lr: 1.5625e-05\n",
      "Epoch 33/300\n",
      "98/98 [==============================] - 60s 617ms/step - loss: 0.2146 - mcrmse: 0.2145 - val_loss: 0.1409 - val_mcrmse: 0.1413 - lr: 1.5625e-05\n",
      "Epoch 34/300\n",
      "98/98 [==============================] - 66s 675ms/step - loss: 0.2050 - mcrmse: 0.2050 - val_loss: 0.1411 - val_mcrmse: 0.1415 - lr: 1.5625e-05\n",
      "Epoch 35/300\n",
      "98/98 [==============================] - 51s 523ms/step - loss: 0.2226 - mcrmse: 0.2225 - val_loss: 0.1406 - val_mcrmse: 0.1408 - lr: 1.5625e-05\n",
      "Epoch 36/300\n",
      "98/98 [==============================] - 56s 571ms/step - loss: 0.2107 - mcrmse: 0.2107 - val_loss: 0.1421 - val_mcrmse: 0.1423 - lr: 1.5625e-05\n",
      "Epoch 37/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2116 - mcrmse: 0.2120\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "98/98 [==============================] - 58s 586ms/step - loss: 0.2116 - mcrmse: 0.2120 - val_loss: 0.1413 - val_mcrmse: 0.1415 - lr: 1.5625e-05\n",
      "Epoch 38/300\n",
      "98/98 [==============================] - 60s 617ms/step - loss: 0.2151 - mcrmse: 0.2149 - val_loss: 0.1400 - val_mcrmse: 0.1403 - lr: 3.9063e-06\n",
      "Epoch 39/300\n",
      "98/98 [==============================] - 61s 621ms/step - loss: 0.2174 - mcrmse: 0.2174 - val_loss: 0.1403 - val_mcrmse: 0.1406 - lr: 3.9063e-06\n",
      "Epoch 40/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2145 - mcrmse: 0.2144\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "98/98 [==============================] - 55s 557ms/step - loss: 0.2145 - mcrmse: 0.2144 - val_loss: 0.1401 - val_mcrmse: 0.1404 - lr: 3.9063e-06\n",
      "Epoch 41/300\n",
      "98/98 [==============================] - 53s 546ms/step - loss: 0.2043 - mcrmse: 0.2044 - val_loss: 0.1399 - val_mcrmse: 0.1402 - lr: 9.7656e-07\n",
      "Epoch 42/300\n",
      "98/98 [==============================] - 57s 587ms/step - loss: 0.2157 - mcrmse: 0.2157 - val_loss: 0.1397 - val_mcrmse: 0.1400 - lr: 9.7656e-07\n",
      "Epoch 43/300\n",
      "98/98 [==============================] - 63s 641ms/step - loss: 0.2165 - mcrmse: 0.2165 - val_loss: 0.1400 - val_mcrmse: 0.1403 - lr: 9.7656e-07\n",
      "Epoch 44/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2144 - mcrmse: 0.2145\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "98/98 [==============================] - 59s 599ms/step - loss: 0.2144 - mcrmse: 0.2145 - val_loss: 0.1399 - val_mcrmse: 0.1401 - lr: 9.7656e-07\n",
      "Epoch 45/300\n",
      "98/98 [==============================] - 55s 557ms/step - loss: 0.2161 - mcrmse: 0.2162 - val_loss: 0.1402 - val_mcrmse: 0.1405 - lr: 2.4414e-07\n",
      "Epoch 46/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2101 - mcrmse: 0.2102\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "98/98 [==============================] - 60s 613ms/step - loss: 0.2101 - mcrmse: 0.2102 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 2.4414e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300\n",
      "98/98 [==============================] - 61s 624ms/step - loss: 0.2155 - mcrmse: 0.2159 - val_loss: 0.1401 - val_mcrmse: 0.1404 - lr: 6.1035e-08\n",
      "Epoch 48/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2161 - mcrmse: 0.2163\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.525878978725359e-08.\n",
      "98/98 [==============================] - 59s 601ms/step - loss: 0.2161 - mcrmse: 0.2163 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 6.1035e-08\n",
      "Epoch 49/300\n",
      "98/98 [==============================] - 57s 586ms/step - loss: 0.2076 - mcrmse: 0.2080 - val_loss: 0.1401 - val_mcrmse: 0.1404 - lr: 1.5259e-08\n",
      "Epoch 50/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2174 - mcrmse: 0.2175\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 3.814697446813398e-09.\n",
      "98/98 [==============================] - 56s 576ms/step - loss: 0.2174 - mcrmse: 0.2175 - val_loss: 0.1407 - val_mcrmse: 0.1409 - lr: 1.5259e-08\n",
      "Epoch 51/300\n",
      "98/98 [==============================] - 56s 577ms/step - loss: 0.2178 - mcrmse: 0.2180 - val_loss: 0.1413 - val_mcrmse: 0.1415 - lr: 3.8147e-09\n",
      "Epoch 52/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2203 - mcrmse: 0.2207\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 9.536743617033494e-10.\n",
      "98/98 [==============================] - 57s 583ms/step - loss: 0.2203 - mcrmse: 0.2207 - val_loss: 0.1403 - val_mcrmse: 0.1406 - lr: 3.8147e-09\n",
      "Epoch 53/300\n",
      "98/98 [==============================] - 58s 594ms/step - loss: 0.2105 - mcrmse: 0.2104 - val_loss: 0.1403 - val_mcrmse: 0.1406 - lr: 9.5367e-10\n",
      "Epoch 54/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2204 - mcrmse: 0.2208\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 2.3841859042583735e-10.\n",
      "98/98 [==============================] - 60s 609ms/step - loss: 0.2204 - mcrmse: 0.2208 - val_loss: 0.1404 - val_mcrmse: 0.1407 - lr: 9.5367e-10\n",
      "Epoch 55/300\n",
      "98/98 [==============================] - 59s 605ms/step - loss: 0.2096 - mcrmse: 0.2098 - val_loss: 0.1407 - val_mcrmse: 0.1410 - lr: 2.3842e-10\n",
      "Epoch 56/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2108 - mcrmse: 0.2108\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 5.960464760645934e-11.\n",
      "98/98 [==============================] - 56s 567ms/step - loss: 0.2108 - mcrmse: 0.2108 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 2.3842e-10\n",
      "Epoch 57/300\n",
      "98/98 [==============================] - 56s 569ms/step - loss: 0.2139 - mcrmse: 0.2139 - val_loss: 0.1407 - val_mcrmse: 0.1410 - lr: 5.9605e-11\n",
      "Epoch 58/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2167 - mcrmse: 0.2168\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.4901161901614834e-11.\n",
      "98/98 [==============================] - 52s 533ms/step - loss: 0.2167 - mcrmse: 0.2168 - val_loss: 0.1408 - val_mcrmse: 0.1410 - lr: 5.9605e-11\n",
      "Epoch 59/300\n",
      "98/98 [==============================] - 54s 555ms/step - loss: 0.2087 - mcrmse: 0.2087 - val_loss: 0.1410 - val_mcrmse: 0.1412 - lr: 1.4901e-11\n",
      "Epoch 60/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2181 - mcrmse: 0.2182\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 3.725290475403709e-12.\n",
      "98/98 [==============================] - 56s 570ms/step - loss: 0.2181 - mcrmse: 0.2182 - val_loss: 0.1407 - val_mcrmse: 0.1409 - lr: 1.4901e-11\n",
      "Epoch 61/300\n",
      "98/98 [==============================] - 54s 554ms/step - loss: 0.2140 - mcrmse: 0.2139 - val_loss: 0.1402 - val_mcrmse: 0.1405 - lr: 3.7253e-12\n",
      "Epoch 62/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2094 - mcrmse: 0.2096\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 9.313226188509272e-13.\n",
      "98/98 [==============================] - 55s 564ms/step - loss: 0.2094 - mcrmse: 0.2096 - val_loss: 0.1404 - val_mcrmse: 0.1407 - lr: 3.7253e-12\n",
      "Epoch 63/300\n",
      "98/98 [==============================] - 55s 558ms/step - loss: 0.2244 - mcrmse: 0.2243 - val_loss: 0.1402 - val_mcrmse: 0.1404 - lr: 9.3132e-13\n",
      "Epoch 64/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2199 - mcrmse: 0.2200\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 2.328306547127318e-13.\n",
      "98/98 [==============================] - 54s 548ms/step - loss: 0.2199 - mcrmse: 0.2200 - val_loss: 0.1397 - val_mcrmse: 0.1400 - lr: 9.3132e-13\n",
      "Epoch 65/300\n",
      "98/98 [==============================] - 52s 527ms/step - loss: 0.2106 - mcrmse: 0.2106 - val_loss: 0.1397 - val_mcrmse: 0.1400 - lr: 2.3283e-13\n",
      "Epoch 66/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2154 - mcrmse: 0.2154\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 5.820766367818295e-14.\n",
      "98/98 [==============================] - 53s 541ms/step - loss: 0.2154 - mcrmse: 0.2154 - val_loss: 0.1397 - val_mcrmse: 0.1400 - lr: 2.3283e-13\n",
      "Epoch 67/300\n",
      "98/98 [==============================] - 54s 550ms/step - loss: 0.2141 - mcrmse: 0.2142 - val_loss: 0.1400 - val_mcrmse: 0.1403 - lr: 5.8208e-14\n",
      "Epoch 68/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2131 - mcrmse: 0.2135\n",
      "Epoch 68: ReduceLROnPlateau reducing learning rate to 1.4551915919545737e-14.\n",
      "98/98 [==============================] - 56s 569ms/step - loss: 0.2131 - mcrmse: 0.2135 - val_loss: 0.1407 - val_mcrmse: 0.1409 - lr: 5.8208e-14\n",
      "Epoch 69/300\n",
      "98/98 [==============================] - 54s 553ms/step - loss: 0.2119 - mcrmse: 0.2118 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 1.4552e-14\n",
      "Epoch 70/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2148 - mcrmse: 0.2147\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 3.637978979886434e-15.\n",
      "98/98 [==============================] - 58s 596ms/step - loss: 0.2148 - mcrmse: 0.2147 - val_loss: 0.1410 - val_mcrmse: 0.1412 - lr: 1.4552e-14\n",
      "Epoch 71/300\n",
      "98/98 [==============================] - 56s 567ms/step - loss: 0.2156 - mcrmse: 0.2159 - val_loss: 0.1410 - val_mcrmse: 0.1413 - lr: 3.6380e-15\n",
      "Epoch 72/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2102 - mcrmse: 0.2103\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 9.094947449716085e-16.\n",
      "98/98 [==============================] - 54s 553ms/step - loss: 0.2102 - mcrmse: 0.2103 - val_loss: 0.1401 - val_mcrmse: 0.1403 - lr: 3.6380e-15\n",
      "Epoch 73/300\n",
      "98/98 [==============================] - 54s 549ms/step - loss: 0.2145 - mcrmse: 0.2145 - val_loss: 0.1399 - val_mcrmse: 0.1402 - lr: 9.0949e-16\n",
      "Epoch 74/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2202 - mcrmse: 0.2200\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 2.2737368624290214e-16.\n",
      "98/98 [==============================] - 51s 524ms/step - loss: 0.2202 - mcrmse: 0.2200 - val_loss: 0.1404 - val_mcrmse: 0.1407 - lr: 9.0949e-16\n",
      "Epoch 75/300\n",
      "98/98 [==============================] - 51s 526ms/step - loss: 0.2138 - mcrmse: 0.2139 - val_loss: 0.1405 - val_mcrmse: 0.1407 - lr: 2.2737e-16\n",
      "Epoch 76/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2153 - mcrmse: 0.2154\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 5.684342156072553e-17.\n",
      "98/98 [==============================] - 51s 522ms/step - loss: 0.2153 - mcrmse: 0.2154 - val_loss: 0.1398 - val_mcrmse: 0.1401 - lr: 2.2737e-16\n",
      "Epoch 77/300\n",
      "98/98 [==============================] - 53s 542ms/step - loss: 0.2177 - mcrmse: 0.2178 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 5.6843e-17\n",
      "Epoch 78/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2213 - mcrmse: 0.2212\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 1.4210855390181384e-17.\n",
      "98/98 [==============================] - 52s 531ms/step - loss: 0.2213 - mcrmse: 0.2212 - val_loss: 0.1400 - val_mcrmse: 0.1403 - lr: 5.6843e-17\n",
      "Epoch 79/300\n",
      "98/98 [==============================] - 52s 528ms/step - loss: 0.2068 - mcrmse: 0.2067 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 1.4211e-17\n",
      "Epoch 80/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2183 - mcrmse: 0.2185\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 3.552713847545346e-18.\n",
      "98/98 [==============================] - 52s 532ms/step - loss: 0.2183 - mcrmse: 0.2185 - val_loss: 0.1401 - val_mcrmse: 0.1404 - lr: 1.4211e-17\n",
      "Epoch 81/300\n",
      "98/98 [==============================] - 54s 546ms/step - loss: 0.2203 - mcrmse: 0.2202 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 3.5527e-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2241 - mcrmse: 0.2240\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 8.881784618863365e-19.\n",
      "98/98 [==============================] - 55s 562ms/step - loss: 0.2241 - mcrmse: 0.2240 - val_loss: 0.1400 - val_mcrmse: 0.1403 - lr: 3.5527e-18\n",
      "Epoch 83/300\n",
      "98/98 [==============================] - 61s 623ms/step - loss: 0.2167 - mcrmse: 0.2169 - val_loss: 0.1395 - val_mcrmse: 0.1398 - lr: 8.8818e-19\n",
      "Epoch 84/300\n",
      "98/98 [==============================] - 56s 576ms/step - loss: 0.2150 - mcrmse: 0.2152 - val_loss: 0.1398 - val_mcrmse: 0.1401 - lr: 8.8818e-19\n",
      "Epoch 85/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2103 - mcrmse: 0.2106\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 2.220446154715841e-19.\n",
      "98/98 [==============================] - 57s 578ms/step - loss: 0.2103 - mcrmse: 0.2106 - val_loss: 0.1402 - val_mcrmse: 0.1404 - lr: 8.8818e-19\n",
      "Epoch 86/300\n",
      "98/98 [==============================] - 54s 553ms/step - loss: 0.2137 - mcrmse: 0.2138 - val_loss: 0.1407 - val_mcrmse: 0.1409 - lr: 2.2204e-19\n",
      "Epoch 87/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2079 - mcrmse: 0.2079\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 5.551115386789603e-20.\n",
      "98/98 [==============================] - 55s 562ms/step - loss: 0.2079 - mcrmse: 0.2079 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 2.2204e-19\n",
      "Epoch 88/300\n",
      "98/98 [==============================] - 54s 556ms/step - loss: 0.2230 - mcrmse: 0.2232 - val_loss: 0.1404 - val_mcrmse: 0.1407 - lr: 5.5511e-20\n",
      "Epoch 89/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2167 - mcrmse: 0.2167\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.3877788466974007e-20.\n",
      "98/98 [==============================] - 57s 583ms/step - loss: 0.2167 - mcrmse: 0.2167 - val_loss: 0.1404 - val_mcrmse: 0.1407 - lr: 5.5511e-20\n",
      "Epoch 90/300\n",
      "98/98 [==============================] - 59s 598ms/step - loss: 0.2173 - mcrmse: 0.2175 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 1.3878e-20\n",
      "Epoch 91/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2142 - mcrmse: 0.2142\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.469447116743502e-21.\n",
      "98/98 [==============================] - 59s 600ms/step - loss: 0.2142 - mcrmse: 0.2142 - val_loss: 0.1402 - val_mcrmse: 0.1405 - lr: 1.3878e-20\n",
      "Epoch 92/300\n",
      "98/98 [==============================] - 55s 562ms/step - loss: 0.2145 - mcrmse: 0.2146 - val_loss: 0.1402 - val_mcrmse: 0.1405 - lr: 3.4694e-21\n",
      "Epoch 93/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2088 - mcrmse: 0.2088\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 8.673617791858755e-22.\n",
      "98/98 [==============================] - 51s 519ms/step - loss: 0.2088 - mcrmse: 0.2088 - val_loss: 0.1405 - val_mcrmse: 0.1407 - lr: 3.4694e-21\n",
      "Epoch 94/300\n",
      "98/98 [==============================] - 52s 527ms/step - loss: 0.2150 - mcrmse: 0.2150 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 8.6736e-22\n",
      "Epoch 95/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2115 - mcrmse: 0.2117\n",
      "Epoch 95: ReduceLROnPlateau reducing learning rate to 2.1684044479646887e-22.\n",
      "98/98 [==============================] - 53s 543ms/step - loss: 0.2115 - mcrmse: 0.2117 - val_loss: 0.1403 - val_mcrmse: 0.1406 - lr: 8.6736e-22\n",
      "Epoch 96/300\n",
      "98/98 [==============================] - 57s 578ms/step - loss: 0.2107 - mcrmse: 0.2106 - val_loss: 0.1402 - val_mcrmse: 0.1405 - lr: 2.1684e-22\n",
      "Epoch 97/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2119 - mcrmse: 0.2118\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 5.421011119911722e-23.\n",
      "98/98 [==============================] - 64s 650ms/step - loss: 0.2119 - mcrmse: 0.2118 - val_loss: 0.1401 - val_mcrmse: 0.1404 - lr: 2.1684e-22\n",
      "Epoch 98/300\n",
      "98/98 [==============================] - 60s 616ms/step - loss: 0.2170 - mcrmse: 0.2170 - val_loss: 0.1410 - val_mcrmse: 0.1413 - lr: 5.4210e-23\n",
      "Epoch 99/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2088 - mcrmse: 0.2088\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 1.3552527799779304e-23.\n",
      "98/98 [==============================] - 58s 586ms/step - loss: 0.2088 - mcrmse: 0.2088 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 5.4210e-23\n",
      "Epoch 100/300\n",
      "98/98 [==============================] - 55s 565ms/step - loss: 0.2270 - mcrmse: 0.2270 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 1.3553e-23\n",
      "Epoch 101/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2142 - mcrmse: 0.2144\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 3.388131949944826e-24.\n",
      "98/98 [==============================] - 52s 528ms/step - loss: 0.2142 - mcrmse: 0.2144 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 1.3553e-23\n",
      "Epoch 102/300\n",
      "98/98 [==============================] - 54s 549ms/step - loss: 0.2056 - mcrmse: 0.2056 - val_loss: 0.1410 - val_mcrmse: 0.1412 - lr: 3.3881e-24\n",
      "Epoch 103/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2170 - mcrmse: 0.2176\n",
      "Epoch 103: ReduceLROnPlateau reducing learning rate to 8.470329874862065e-25.\n",
      "98/98 [==============================] - 53s 543ms/step - loss: 0.2170 - mcrmse: 0.2176 - val_loss: 0.1415 - val_mcrmse: 0.1418 - lr: 3.3881e-24\n",
      "Epoch 104/300\n",
      "98/98 [==============================] - 53s 545ms/step - loss: 0.2133 - mcrmse: 0.2132 - val_loss: 0.1413 - val_mcrmse: 0.1416 - lr: 8.4703e-25\n",
      "Epoch 105/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2156 - mcrmse: 0.2160\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 2.1175824687155163e-25.\n",
      "98/98 [==============================] - 58s 588ms/step - loss: 0.2156 - mcrmse: 0.2160 - val_loss: 0.1405 - val_mcrmse: 0.1408 - lr: 8.4703e-25\n",
      "Epoch 106/300\n",
      "98/98 [==============================] - 59s 600ms/step - loss: 0.2233 - mcrmse: 0.2233 - val_loss: 0.1406 - val_mcrmse: 0.1409 - lr: 2.1176e-25\n",
      "Epoch 107/300\n",
      "98/98 [==============================] - ETA: 0s - loss: 0.2085 - mcrmse: 0.2084\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 5.293956171788791e-26.\n",
      "98/98 [==============================] - 55s 565ms/step - loss: 0.2085 - mcrmse: 0.2084 - val_loss: 0.1399 - val_mcrmse: 0.1402 - lr: 2.1176e-25\n",
      "Epoch 108/300\n",
      "98/98 [==============================] - 56s 577ms/step - loss: 0.2081 - mcrmse: 0.2084 - val_loss: 0.1402 - val_mcrmse: 0.1405 - lr: 5.2940e-26\n",
      "Epoch 108: early stopping\n"
     ]
    }
   ],
   "source": [
    "hitory1=model.fit(x=X_train,y=y_train,epochs=300,batch_size=32,validation_data=(X_test, y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IG6FXaFmmdB8",
    "outputId": "ed9fe9d7-b41e-40a8-c7df-1f34f384d2cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 4s 107ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbA64eigmjAM",
    "outputId": "edd7d61e-83fe-41af-ed0e-5f67b163c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.060544  2.8073852 3.1038997 2.9284852 2.7050638 2.3144805]\n",
      " [4.2346964 3.9889119 3.8833306 4.403566  3.6411748 3.9725156]\n",
      " [3.2831786 3.152833  3.278423  3.1306183 3.1861482 3.7059994]\n",
      " ...\n",
      " [3.336904  2.406166  3.1063833 2.6997616 2.6479027 3.262218 ]\n",
      " [3.4932804 3.4040241 3.9232392 3.5167248 2.963857  3.5503461]\n",
      " [2.5805817 2.5822735 2.6630604 2.996856  2.1830955 2.1393182]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016994304263453806\n",
      "0.02690464616531071\n",
      "0.023637707888579624\n",
      "0.016834125368980116\n",
      "0.02222512603538461\n",
      "0.015453030787441977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "for i in range(6):\n",
    "    error = mean_squared_error(y_test[:,i],y_pred[:,i])\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j84ggnJNncta"
   },
   "outputs": [],
   "source": [
    "sample=pd.read_csv(\"/content/drive/MyDrive/LSTM_network/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final=np.ones((3,6),dtype=float)\n",
    "X_test_final=[test_final_pad,test_final_pad,test_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"model_trial.json\",\"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights_trial.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model and weights\n",
    "json_file=open('model_trial.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"weights_trial.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 12s 124ms/step\n",
      "(3128, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_rand = [train_com_pad,train_com_pad,train_s];\n",
    "y_pred=model.predict(X_train_rand)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011125922495189481\n",
      "0.015309660592061305\n",
      "0.01340009335178853\n",
      "0.010513719649870733\n",
      "0.014354437100536134\n",
      "0.0085495108602342\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    error = mean_squared_error(train_s[:,i],y_pred[:,i])\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "MTuTXNeHntIB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2007238  1.1055958  1.5844516  1.6045799  1.1289417  1.7898675 ]\n",
      " [0.76105666 0.70036066 0.9543434  0.92761356 0.6785991  1.0206932 ]\n",
      " [1.0990517  1.1511546  1.6719697  1.534158   1.068452   1.7473599 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8UjchTunv-e"
   },
   "outputs": [],
   "source": [
    "sample.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
